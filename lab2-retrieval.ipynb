{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6472e038",
      "metadata": {
        "id": "6472e038"
      },
      "source": [
        "# üîç FAISS Similarity Search with Sentence Transformers\n",
        "This notebook loads academic papers from arXiv (PDFs), extracts text, chunks and embeds it using SentenceTransformers, and performs similarity search using FAISS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a257036f",
      "metadata": {
        "id": "a257036f"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers pymupdf faiss-cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45a0e0ce",
      "metadata": {
        "id": "45a0e0ce"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import fitz  # PyMuPDF\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b2ca0f3",
      "metadata": {
        "id": "4b2ca0f3"
      },
      "outputs": [],
      "source": [
        "# --- Step 1: Download PDFs from arXiv ---\n",
        "pdf_urls = [\n",
        "    \"https://arxiv.org/pdf/2401.15884\",\n",
        "    \"https://arxiv.org/pdf/2005.11401\"\n",
        "]\n",
        "\n",
        "local_paths = []\n",
        "for i, url in enumerate(pdf_urls):\n",
        "    response = requests.get(url)\n",
        "    filename = f\"paper_{i}.pdf\"\n",
        "    with open(filename, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    local_paths.append(filename)\n",
        "\n",
        "print(f\"Downloaded {len(local_paths)} PDFs.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80d02af3",
      "metadata": {
        "id": "80d02af3"
      },
      "outputs": [],
      "source": [
        "# --- Step 2: Extract text using PyMuPDF ---\n",
        "def extract_text_from_pdf(path):\n",
        "    doc = fitz.open(path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "texts = [extract_text_from_pdf(path) for path in local_paths]\n",
        "full_text = \"\\n\".join(texts)\n",
        "print(f\"Total extracted characters: {len(full_text)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff4a08a4",
      "metadata": {
        "id": "ff4a08a4"
      },
      "outputs": [],
      "source": [
        "# --- Step 3: Split text into overlapping chunks ---\n",
        "def split_text(text, chunk_size=500, overlap=100):\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = min(start + chunk_size, len(text))\n",
        "        chunks.append(text[start:end])\n",
        "        start += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "chunks = split_text(full_text)\n",
        "print(f\"Generated {len(chunks)} chunks.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6378446b",
      "metadata": {
        "id": "6378446b"
      },
      "outputs": [],
      "source": [
        "# --- Step 4: Generate embeddings using SentenceTransformer ---\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = model.encode(chunks, convert_to_numpy=True, show_progress_bar=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be28c947",
      "metadata": {
        "id": "be28c947"
      },
      "outputs": [],
      "source": [
        "# --- Step 5: Build FAISS index ---\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(embeddings)\n",
        "print(f\"FAISS index built with {index.ntotal} vectors.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b52ebcae",
      "metadata": {
        "id": "b52ebcae"
      },
      "outputs": [],
      "source": [
        "# --- Step 6: Similarity search interface ---\n",
        "def search(query, k=3):\n",
        "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "    print(f\"\\nTop {k} results for: '{query}'\\n\")\n",
        "    for i, idx in enumerate(indices[0]):\n",
        "        print(f\"Result {i+1} (score={distances[0][i]:.2f}):\\n{chunks[idx][:500]}\\n{'-'*80}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e217801",
      "metadata": {
        "collapsed": true,
        "id": "4e217801"
      },
      "outputs": [],
      "source": [
        "# --- Step 7: Try some sample questions based on the paper ---\n",
        "sample_questions = [\n",
        "    \"What is RAG and how does it work?\",\n",
        "    \"What is the difference between RAG-Sequence and RAG-Token?\",\n",
        "    \"How does RAG use non-parametric memory?\",\n",
        "    \"What tasks were used to evaluate RAG?\",\n",
        "    \"How is Dense Passage Retrieval (DPR) used in RAG?\",\n",
        "    \"What is the advantage of hybrid models over purely parametric models?\",\n",
        "    \"What decoding strategies are used in RAG?\",\n",
        "    \"How does RAG compare to T5 and BART?\",\n",
        "    \"What datasets were used to benchmark RAG models?\",\n",
        "    \"Can RAG models be updated without retraining?\"\n",
        "]\n",
        "\n",
        "for q in sample_questions:\n",
        "    search(q, k=2)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}